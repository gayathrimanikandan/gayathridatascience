{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Articles_Web_Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QjNSoc5AfbS0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PlE8LIK2lDE",
        "colab_type": "text"
      },
      "source": [
        "## **Articles Web Scraping using SELENIUM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QzbWh8qTuMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Downloading required Libraries.\n",
        "\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbKbZsMM5L6l",
        "colab_type": "text"
      },
      "source": [
        "*Scraping webpages using Selenium and chromium webdriver*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptkwBT555ZNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing required packages\n",
        "\n",
        "import selenium\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support.expected_conditions import presence_of_element_located\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaXbpTSJ79eD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# options for the\n",
        "options = Options()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.binary_location = '/usr/bin/chromium-browser'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kox5AGZF59Pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of urls to be scraped\n",
        "\n",
        "articles = [ #Place the urls to be scraped ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ggEYxlDdbA",
        "colab_type": "text"
      },
      "source": [
        "Using selenium & chrome browser we are scraping the data and using BeautifulSoup we are extracting the article content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBmUjNuQEF0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive = [None] * len(articles)\n",
        "content = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heAiSMmBZKAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# looping the urls to scrape the webpages\n",
        "\n",
        "for i, item in enumerate(articles):\n",
        "\n",
        "    # Retrieve the url in the browser\n",
        "\n",
        "    drive[i] = webdriver.Chrome(executable_path='/usr/bin/chromedriver', options = options)\n",
        "    drive[i].get(item)\n",
        "\n",
        "    # Set timeout time until it gets the element\n",
        "\n",
        "    WebDriverWait(drive[i], 5).until(\n",
        "        lambda s: s.find_element_by_class_name(\"article-body\").is_displayed()\n",
        "    )\n",
        "    \n",
        "    # Executing the script and closing the driver\n",
        "\n",
        "    res = drive[i].execute_script(\"return document.documentElement.outerHTML\")\n",
        "    drive[i].quit()\n",
        "\n",
        "    # Using BeautifulSoup we get html content of the class article-body and get the text out of it.\n",
        "    \n",
        "    soup = BeautifulSoup(res, 'lxml')\n",
        "    article_content = soup.find('div', {'class': 'article-body'})\n",
        "    text_data = article_content.get_text()\n",
        "\n",
        "    #appending the extracted text into list\n",
        "    \n",
        "    content.append(text_data)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEYBeMPlrtQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}